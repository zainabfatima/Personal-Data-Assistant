ğŸ¤– Personal LLM Data Assistant
A personalized AI assistant that can read, understand, and answer questions about your own documents and datasets using Large Language Models (LLMs).
It combines LangChain, OpenAI GPT models, and vector databases to give you context-aware answers from your local data.

ğŸš€ Features
ğŸ“‚ Document Ingestion â€“ Upload PDFs, TXT, or CSV files for analysis.

ğŸ§¹ Text Processing â€“ Automatic text cleaning and chunking for better retrieval.

ğŸ§  Vector Search â€“ Store embeddings in a vector database for semantic search.

ğŸ’¬ LLM-Powered QA â€“ Use GPT-based models to answer questions about your files.

ğŸ” Semantic Retrieval â€“ Finds the most relevant text chunks for your query.

ğŸŒ Streamlit Interface â€“ Clean and interactive web-based UI.

ğŸ“ Project Structure
graphql
Copy
Edit
Personal-LLM-Data-Assistant/
â”œâ”€â”€ app.py                        # Main Streamlit application
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ utils.py                       # Helper functions (loading, chunking, embedding)
â”œâ”€â”€ vectorstore/                   # Stores FAISS index files
â”œâ”€â”€ data/                          # Folder for uploaded documents
â””â”€â”€ README.md                      # Documentation
âš™ How It Works
Document Loading â€“ Uses libraries like PyPDF2 and pandas to read files.

Chunking â€“ Splits large documents into smaller overlapping chunks using LangChainâ€™s RecursiveCharacterTextSplitter.

Embedding â€“ Converts chunks into numerical vectors via OpenAIEmbeddings.

Vector Storage â€“ Saves vectors in FAISS for fast semantic search.

Query Processing â€“ When you ask a question, it:

Searches FAISS for the most relevant chunks

Sends them to GPT-3.5 or GPT-4

Returns a concise, context-based answer.

ğŸ“¤ Input
.pdf, .txt, .csv files containing your personal or project data.

ğŸ“ˆ Output
Context-aware answers about your files.

Extracted insights and summaries.
